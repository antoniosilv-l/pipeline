# Pipeline de Dados  üé¢

Projeto voltado para o estudo e cria√ß√£o de um pipeline local end-to-end.

Ap√≥s a configura√ß√£o teste e aprendizado local, migraremos a solu√ß√£o para um ambiente docker, onde ser√° mais facil ser reproduzido.


## Airflow üåÄ

Para o processo de Extract and Load, vamos utilizar a solu√ß√£o do apache-airflow.

*Configura√ß√£o*
```bash
# Localiza√ß√£o da pasta home do airflow:
export AIRFLOW_HOME=caminha_da_pasta

# Instala√ß√£o do airflow:
# Seguindo a documenta√ß√£o optei por fazer a instala√ß√£o via CONSTRAINT.
AIRFLOW_VERSION=2.7.3

PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"

CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}
```

*Inicializando o Airflow*

```bash
# Iniciando o banco de dados
airflow db migrate

# Variavel para conex√£o ao banco de dados.
# Para seguran√ßa optei por utilizar a variavel 
# AIRFLOW__DATABASE__SQL_ALCHEMY_CONN para passar a informa√ß√£o do banco de dados.

export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sql_alchemy_conn

# Criando o usuario admin
airflow users create \
    --username admin \
    --firstname Peter \
    --lastname Parker \
    --role Admin \
    --email spiderman@superhero.org

# Iniciando o webserver e scheduler
airflow webserver --port 8080

airflow scheduler
```

## dbt üóùÔ∏è